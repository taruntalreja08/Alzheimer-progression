{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4bVdD4qOAvo"
   },
   "source": [
    "# Classification of Alzheimer's Diseases using Quantification of HippoCampal Volume\n",
    "\n",
    "### Exploratory Data Analysis - Section 2 - HippoCampusDataLoader\n",
    "\n",
    "This task involves building recursive UNet model, training, logging and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "rCNPU4ehgAqk",
    "outputId": "3a5951c0-a0bd-4de6-9dd6-c71ba86e9546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting medpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/70/c1fd5dd60242eee81774696ea7ba4caafac2bad8f028bba94b1af83777d7/MedPy-0.4.0.tar.gz (151kB)\n",
      "\r",
      "\u001b[K     |██▏                             | 10kB 22.1MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 20kB 1.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 30kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 40kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 51kB 1.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 61kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 71kB 2.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 81kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 92kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 102kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 112kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 122kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 133kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 143kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 153kB 2.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from medpy) (1.18.4)\n",
      "Collecting SimpleITK>=1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d8/53338c34f71020725ffb3557846c80af96c29c03bc883551a2565aa68a7c/SimpleITK-1.2.4-cp36-cp36m-manylinux1_x86_64.whl (42.5MB)\n",
      "\u001b[K     |████████████████████████████████| 42.5MB 103kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: medpy\n",
      "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for medpy: filename=MedPy-0.4.0-cp36-cp36m-linux_x86_64.whl size=753430 sha256=4db9af78623b2e80d5b8be8ded5b1d0ba5c3d87bb9fea1fbc5404fb115c57920\n",
      "  Stored in directory: /root/.cache/pip/wheels/8c/c9/9c/2c6281c7a72b9fb1570862a4f028af7ce38405008354fbf870\n",
      "Successfully built medpy\n",
      "Installing collected packages: SimpleITK, medpy\n",
      "Successfully installed SimpleITK-1.2.4 medpy-0.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install medpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AlK3B15vNxab"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "from medpy.io import load\n",
    "\n",
    "#from utils.utils import med_reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbEnQy7FgauM"
   },
   "source": [
    "This module loads the hippocampus dataset into RAM. Note that the data is small enough to fit into the RAM. If not, we need to enable caching techniques. You could enable help using the pytorch community here: https://discuss.pytorch.org/t/best-practice-to-cache-the-entire-dataset-during-first-epoch/19608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSinLvvDgOfO"
   },
   "outputs": [],
   "source": [
    "def LoadHippocampusData(root_dir, y_shape, z_shape):\n",
    "    '''\n",
    "    This function loads our dataset form disk into memory,\n",
    "    reshaping output to common size\n",
    "\n",
    "    Arguments:\n",
    "        volume {Numpy array} -- 3D array representing the volume\n",
    "\n",
    "    Returns:\n",
    "        Array of dictionaries with data stored in seg and image fields as \n",
    "        Numpy arrays of shape [AXIAL_WIDTH, Y_SHAPE, Z_SHAPE]\n",
    "    '''\n",
    "\n",
    "    image_dir = os.path.join(root_dir + \"/output/\", 'images')\n",
    "    label_dir = os.path.join(root_dir + \"/output/\", 'labels')\n",
    "\n",
    "    images = [f for f in listdir(image_dir) if (\n",
    "        isfile(join(image_dir, f)) and f[0] != \".\")]\n",
    "\n",
    "    out = []\n",
    "    for f in images:\n",
    "\n",
    "        # We would benefit from mmap load method here if dataset doesn't fit into memory\n",
    "        # Images are loaded here using MedPy's load method. We will ignore header \n",
    "        # since we will not use it\n",
    "        image, _ = load(os.path.join(image_dir, f))\n",
    "        label, _ = load(os.path.join(label_dir, f))\n",
    "\n",
    "        # Normalize all images (but not labels) so that values are in [0..1] range\n",
    "        # Pixel Normalisation - There are multiple ways of performing the same\n",
    "        \"\"\" Reference: https://machinelearningmastery.com/how-to-manually-scale-image-pixel-data-for-deep-learning/\"\"\"\n",
    "        pixels = np.asarray(image)\n",
    "        pixels = pixels.astype('float32')\n",
    "        pixels /= 255.0\n",
    "\n",
    "        # We need to reshape data since CNN tensors that represent minibatches\n",
    "        # in our case will be stacks of slices and stacks need to be of the same size.\n",
    "        # In the inference pathway we will need to crop the output to that\n",
    "        # of the input image.\n",
    "        # Note that since we feed individual slices to the CNN, we only need to \n",
    "        # extend 2 dimensions out of 3. We choose to extend coronal and sagittal here\n",
    "\n",
    "        image = med_reshape(image, new_shape=(image.shape[0], y_shape, z_shape))\n",
    "        label = med_reshape(label, new_shape=(label.shape[0], y_shape, z_shape)).astype(int)\n",
    "\n",
    "        # Why do we need to cast label to int?\n",
    "        # ANSWER: \n",
    "        \"\"\"Casting helps reduce memory consumption.\"\"\"\n",
    "\n",
    "        out.append({\"image\": image, \"seg\": label, \"filename\": f})\n",
    "\n",
    "    # Hippocampus dataset only takes about 300 Mb RAM, so we can afford to keep it all in RAM\n",
    "    print(f\"Processed {len(out)} files, total {sum([x['image'].shape[0] for x in out])} slices\")\n",
    "    return np.array(out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPO0peTKqq2WeL6MjWF59TF",
   "collapsed_sections": [],
   "name": "Untitled9.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
