{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4bVdD4qOAvo"
   },
   "source": [
    "# Classification of Alzheimer's Diseases using Quantification of HippoCampal Volume\n",
    "\n",
    "### Exploratory Data Analysis - Section 2 - UNetExperiment\n",
    "\n",
    "This task involves building recursive UNet model, training, logging and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g08K4QTqjIuR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#from data_prep.SlicesDataset import SlicesDataset\n",
    "#from utils.utils import log_to_tensorboard\n",
    "#from utils.volume_stats import Dice3d, Jaccard3d\n",
    "#from networks.RecursiveUNet import UNet\n",
    "#from inference.UNetInferenceAgent import UNetInferenceAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbldBKXNjQLG"
   },
   "source": [
    "This module a UNet experiment and contains a class that handles\n",
    "the experiment lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aodxtbjcjNJm"
   },
   "outputs": [],
   "source": [
    "class UNetExperiment:\n",
    "    \"\"\"\n",
    "    This class implements the basic life cycle for a segmentation task with UNet(https://arxiv.org/abs/1505.04597).\n",
    "    The basic life cycle of a UNetExperiment is:\n",
    "\n",
    "        run():\n",
    "            for epoch in n_epochs:\n",
    "                train()\n",
    "                validate()\n",
    "        test()\n",
    "    \"\"\"\n",
    "    def __init__(self, config, split, dataset):\n",
    "        self.n_epochs = config.n_epochs\n",
    "        self.split = split\n",
    "        self._time_start = \"\"\n",
    "        self._time_end = \"\"\n",
    "        self.epoch = 0\n",
    "        self.name = config.name\n",
    "\n",
    "        # Create output folders\n",
    "        dirname = f'{time.strftime(\"%Y-%m-%d_%H%M\", time.gmtime())}_{self.name}'\n",
    "        self.out_dir = os.path.join(config.test_results_dir, dirname)\n",
    "        os.makedirs(self.out_dir, exist_ok=True)\n",
    "\n",
    "        # Create data loaders\n",
    "        # SlicesDataset class is not complete. Go to the file and complete it. \n",
    "        # Note that we are using a 2D version of UNet here, which means that it will expect\n",
    "        # batches of 2D slices.\n",
    "        self.train_loader = DataLoader(SlicesDataset(dataset[split[\"train\"]]),\n",
    "                batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "        self.val_loader = DataLoader(SlicesDataset(dataset[split[\"val\"]]),\n",
    "                batch_size=config.batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        # we will access volumes directly for testing\n",
    "        self.test_data = dataset[split[\"test\"]]\n",
    "\n",
    "        # Do we have CUDA available?\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"WARNING: No CUDA device is found. This may take significantly longer!\")\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Configure our model and other training implements\n",
    "        # We will use a recursive UNet model from German Cancer Research Center, \n",
    "        # Division of Medical Image Computing. It is quite complicated and works \n",
    "        # very well on this task. Feel free to explore it or plug in your own model\n",
    "        self.model = UNet(num_classes=3)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # We are using a standard cross-entropy loss since the model output is essentially\n",
    "        # a tensor with softmax'd prediction of each pixel's probability of belonging \n",
    "        # to a certain class\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # We are using standard SGD method to optimize our weights\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.learning_rate)\n",
    "        # Scheduler helps us update learning rate automatically\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # Set up Tensorboard. By default it saves data into runs folder. You need to launch\n",
    "        self.tensorboard_train_writer = SummaryWriter(comment=\"_train\")\n",
    "        self.tensorboard_val_writer = SummaryWriter(comment=\"_val\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        This method is executed once per epoch and takes \n",
    "        care of model weight update cycle\n",
    "        \"\"\"\n",
    "        print(f\"Training epoch {self.epoch}...\")\n",
    "        self.model.train()\n",
    "\n",
    "        # Loop over our minibatches\n",
    "        for i, batch in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # You have your data in batch variable. Put the slices as 4D Torch Tensors of \n",
    "            # shape [BATCH_SIZE, 1, PATCH_SIZE, PATCH_SIZE] into variables data and target. \n",
    "            # Feed data to the model and feed target to the loss function\n",
    "          \n",
    "            \"\"\" data should have 4d torch tensor of batch\"\"\"\n",
    "            data = batch[\"image\"]\n",
    "            #data = data.unsqueeze(0) # data size should be 1, patch, patch\n",
    "            #data = torch.stack(data)\n",
    "            #data = data.reshape([-1, 1, -1, -1])\n",
    "\n",
    "            \"\"\" target should have 4d torch tensor of batch\"\"\"\n",
    "            target = batch[\"seg\"]\n",
    "            #target = target.unsqueeze(0) # data size should be 1, patch, patch\n",
    "            #target = torch.stack(target)\n",
    "\n",
    "            prediction = self.model(data.to(self.device))\n",
    "\n",
    "            # We are also getting softmax'd version of prediction to output a probability map\n",
    "            # so that we can see how the model converges to the solution\n",
    "            prediction_softmax = F.softmax(prediction, dim=1)\n",
    "            \n",
    "            #Typecasting of loss to long is necessary\n",
    "            loss = self.loss_function(prediction, target[:, 0, :, :].long().to(self.device))\n",
    " \n",
    "            # TASK: What does each dimension of variable prediction represent?\n",
    "            \"\"\"\n",
    "            [8, 3, 64, 64]\n",
    "            64, 64 is the 2d slice dimension\n",
    "            8 is the batch size\n",
    "            and 3 is the class probability\n",
    "            \"\"\"\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if (i % 10) == 0:\n",
    "                # Output to console on every 10th batch\n",
    "                print(f\"\\nEpoch: {self.epoch} Train loss: {loss}, {100*(i+1)/len(self.train_loader):.1f}% complete\")\n",
    "\n",
    "                counter = 100*self.epoch + 100*(i/len(self.train_loader))\n",
    "\n",
    "                # You don't need to do anything with this function, but you are welcome to \n",
    "                # check it out if you want to see how images are logged to Tensorboard\n",
    "                # or if you want to output additional debug data\n",
    "                log_to_tensorboard(\n",
    "                    self.tensorboard_train_writer,\n",
    "                    loss,\n",
    "                    data,\n",
    "                    target,\n",
    "                    prediction_softmax,\n",
    "                    prediction,\n",
    "                    counter)\n",
    "\n",
    "            print(\".\", end='')\n",
    "\n",
    "        print(\"\\nTraining complete\")\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        This method runs validation cycle, using same metrics as \n",
    "        Train method. Note that model needs to be switched to eval\n",
    "        mode and no_grad needs to be called so that gradients do not \n",
    "        propagate\n",
    "        \"\"\"\n",
    "        print(f\"Validating epoch {self.epoch}...\")\n",
    "\n",
    "        # Turn off gradient accumulation by switching model to \"eval\" mode\n",
    "        self.model.eval()\n",
    "        loss_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(self.val_loader):\n",
    "                \n",
    "                # Write validation code that will compute loss on a validation sample\n",
    "                dev_iter.init_epoch()\n",
    "                with torch.no_grad():\n",
    "                    for dev_batch_index, dev_batch in enumerate(dev_iter):\n",
    "                        answer = model(dev_batch)\n",
    "                        loss = criterion(answer, dev_batch.label)\n",
    "\n",
    "                print(f\"Batch {i}. Data shape {data.shape} Loss {loss}\")\n",
    "\n",
    "                # We report loss that is accumulated across all of validation set\n",
    "                loss_list.append(loss.item())\n",
    "\n",
    "        self.scheduler.step(np.mean(loss_list))\n",
    "\n",
    "        log_to_tensorboard(\n",
    "            self.tensorboard_val_writer,\n",
    "            np.mean(loss_list),\n",
    "            data,\n",
    "            target,\n",
    "            prediction_softmax, \n",
    "            prediction,\n",
    "            (self.epoch+1) * 100)\n",
    "        print(f\"Validation complete\")\n",
    "\n",
    "    def save_model_parameters(self):\n",
    "        \"\"\"\n",
    "        Saves model parameters to a file in results directory\n",
    "        \"\"\"\n",
    "        path = os.path.join(self.out_dir, \"model.pth\")\n",
    "\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    def load_model_parameters(self, path=''):\n",
    "        \"\"\"\n",
    "        Loads model parameters from a supplied path or a\n",
    "        results directory\n",
    "        \"\"\"\n",
    "        if not path:\n",
    "            model_path = os.path.join(self.out_dir, \"model.pth\")\n",
    "        else:\n",
    "            model_path = path\n",
    "\n",
    "        if os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path))\n",
    "        else:\n",
    "            raise Exception(f\"Could not find path {model_path}\")\n",
    "\n",
    "    def run_test(self):\n",
    "        \"\"\"\n",
    "        This runs test cycle on the test dataset.\n",
    "        Note that process and evaluations are quite different\n",
    "        Here we are computing a lot more metrics and returning\n",
    "        a dictionary that could later be persisted as JSON\n",
    "        \"\"\"\n",
    "        print(\"Testing...\")\n",
    "        self.model.eval()\n",
    "\n",
    "        # In this method we will be computing metrics that are relevant to the task of 3D volume\n",
    "        # segmentation. Therefore, unlike train and validation methods, we will do inferences\n",
    "        # on full 3D volumes, much like we will be doing it when we deploy the model in the \n",
    "        # clinical environment. \n",
    "\n",
    "        # Inference Agent is not complete. Go and finish it. Feel free to test the class\n",
    "        # in a module of your own by running it against one of the data samples\n",
    "        inference_agent = UNetInferenceAgent(model=self.model, device=self.device)\n",
    "\n",
    "        out_dict = {}\n",
    "        out_dict[\"volume_stats\"] = []\n",
    "        dc_list = []\n",
    "        jc_list = []\n",
    "\n",
    "        # for every in test set\n",
    "        for i, x in enumerate(self.test_data):\n",
    "            pred_label = inference_agent.single_volume_inference(x[\"image\"])\n",
    "\n",
    "            # We compute and report Dice and Jaccard similarity coefficients which \n",
    "            # assess how close our volumes are to each other\n",
    "\n",
    "            dc = Dice3d(pred_label, x[\"seg\"])\n",
    "\n",
    "            # Jaccard3D function is not implemented. Complete the implementation.\n",
    "            # You can look up the definition of Jaccard on Wikipedia. If you completed it\n",
    "            # correctly (and if you picked your train/val/test split right ;)),\n",
    "            # your average Jaccard on your validation set should be around 0.80\n",
    "            jc = Jaccard3d(pred_label, x[\"seg\"])\n",
    "            dc_list.append(dc)\n",
    "            jc_list.append(jc)\n",
    "\n",
    "            # STAND-OUT SUGGESTION: By way of exercise, consider also outputting:\n",
    "            # * Sensitivity and specificity (and explain semantic meaning in terms of \n",
    "            #   under/over segmenting)\n",
    "            # * Dice-per-slice and render combined slices with lowest and highest DpS\n",
    "            # * Dice per class (anterior/posterior)\n",
    "\n",
    "            out_dict[\"volume_stats\"].append({\n",
    "                \"filename\": x['filename'],\n",
    "                \"dice\": dc,\n",
    "                \"jaccard\": jc\n",
    "                })\n",
    "            print(f\"{x['filename']} Dice {dc:.4f}. {100*(i+1)/len(self.test_data):.2f}% complete\")\n",
    "\n",
    "        out_dict[\"overall\"] = {\n",
    "            \"mean_dice\": np.mean(dc_list),\n",
    "            \"mean_jaccard\": np.mean(jc_list)}\n",
    "\n",
    "        print(\"\\nTesting complete.\")\n",
    "        return out_dict\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Kicks off train cycle and writes model parameter file at the end\n",
    "        \"\"\"\n",
    "        self._time_start = time.time()\n",
    "\n",
    "        print(\"Experiment started.\")\n",
    "\n",
    "        # Iterate over epochs\n",
    "        for self.epoch in range(self.n_epochs):\n",
    "            self.train()\n",
    "            self.validate()\n",
    "\n",
    "        # save model for inferencing\n",
    "        self.save_model_parameters()\n",
    "\n",
    "        self._time_end = time.time()\n",
    "        print(f\"Run complete. Total time: {time.strftime('%H:%M:%S', time.gmtime(self._time_end - self._time_start))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNo0MFrV/+X1Cho6eGB3o5r",
   "collapsed_sections": [],
   "name": "Untitled9.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
